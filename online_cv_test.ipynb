{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Dec 23 15:07:09 2019\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def optF1Score( pt,y ):\n",
    "    '''\n",
    "    # 针对macro-f1进行结果的优化\n",
    "    ool = [N_trainSample,N_label], N_trainSample为训练集样本个数，N_label为标签个数\n",
    "    eg:\n",
    "        ool = array(\n",
    "        [[0.49505357, 0.08540789, 0.41953854],\n",
    "        [0.39238397, 0.21128543, 0.3963306 ],\n",
    "        [0.45673946, 0.12019312, 0.42306742],\n",
    "        [0.57948197, 0.07797233, 0.34254569]])\n",
    "    y: [N_trainSample,1]\n",
    "    eg: [1,2,1,0]\n",
    "    \n",
    "    '''\n",
    "    from scipy import optimize\n",
    "    \n",
    "    pt = np.array(pt)\n",
    "    def fun(x):\n",
    "        tmp = np.hstack(\n",
    "                [x[0] * pt[:, 0].reshape(-1, 1), \n",
    "                 x[1] * pt[:, 1].reshape(-1, 1), \n",
    "                 x[2] * pt[:, 2].reshape(-1, 1)])\n",
    "        return - f1_score( np.array(y), np.argmax(tmp, axis=1), average='macro' )\n",
    "    initW = np.asarray((1,1,1))\n",
    "    \n",
    "    return optimize.fmin_powell(fun, initW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.733862\n",
      "         Iterations: 3\n",
      "         Function evaluations: 303\n"
     ]
    }
   ],
   "source": [
    "oof_train = pd.read_csv('cache/opt-f1score.csv')\n",
    "pWeight = optF1Score( oof_train[['p1', 'p2', 'p3']],oof_train['y'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.941132</td>\n",
       "      <td>0.461139</td>\n",
       "      <td>-0.294789</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.674449</td>\n",
       "      <td>1.630520</td>\n",
       "      <td>-1.304830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.581116</td>\n",
       "      <td>2.886493</td>\n",
       "      <td>-1.065741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.278459</td>\n",
       "      <td>-0.212601</td>\n",
       "      <td>2.951491</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.658640</td>\n",
       "      <td>1.836897</td>\n",
       "      <td>-0.116195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         p1        p2        p3  y\n",
       "0 -0.941132  0.461139 -0.294789  1\n",
       "1 -1.674449  1.630520 -1.304830  1\n",
       "2 -2.581116  2.886493 -1.065741  1\n",
       "3 -3.278459 -0.212601  2.951491  2\n",
       "4 -2.658640  1.836897 -0.116195  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.21294424, 0.87312334, 1.11360587])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from modeling import Classification\n",
    "from bert import tokenization\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert_single_example( tokenizer, text_a ):\n",
    "\n",
    "  tokens_a = tokenizer.tokenize(text_a)\n",
    "  tokens = []\n",
    "  tokens.append(\"[CLS]\")\n",
    "  for token in tokens_a:\n",
    "    tokens.append(token)\n",
    "  tokens.append(\"[SEP]\")\n",
    "\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)# 将中文转换成ids\n",
    "  input_mask = [1] * len(input_ids) # 创建mask\n",
    "\n",
    "  return tokens,input_ids,input_mask # 对应的就是创建bert模型时候的input_ids,input_mask,segment_ids 参数\n",
    "\n",
    "def process(data, args, tokenizer):\n",
    "    # 数据预处理\n",
    "    T1 = []\n",
    "    T2 = []\n",
    "    for d in tqdm(data.iterrows()):\n",
    "        d = d[1]\n",
    "        text = str(d['微博中文内容'])\n",
    "        text = text[-(args.max_x_length-2):]\n",
    "        tokens,t1,t2 = convert_single_example( tokenizer, text )\n",
    "        T1.append( list(t1)+[0] * (args.max_x_length - len(list(t1))) )\n",
    "        T2.append( list(t2)+[0] * (args.max_x_length - len(list(t2))) )\n",
    "        \n",
    "    return T1,T2\n",
    "\n",
    "def predict( T1,T2, model ):\n",
    "    rs = []\n",
    "    bs = 200\n",
    "    num = int( len(T1)/bs ) + 1\n",
    "    for n in tqdm(range(num)):\n",
    "        T1_ = T1[n*bs:(n+1)*bs]\n",
    "        T2_ = T2[n*bs:(n+1)*bs]\n",
    "        if len(T1_)>0:\n",
    "            feed = { model.t1_:T1_, model.t2_:T2_ }\n",
    "            _c = sess.run([model.logits], feed_dict=feed)[0]\n",
    "            rs.extend(_c)\n",
    "    return np.array(rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \n",
    "    batchSize = 32\n",
    "    max_x_length = 140\n",
    "    learning_rate = 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "args = Config()\n",
    "\n",
    "output_path = '../submit/Result_0326_1.csv'\n",
    "\n",
    "bert_vocab_file = './chinese_roberta_wwm_ext_L-12_H-768_A-12/vocab.txt'\n",
    "tokenizer = tokenization.FullTokenizer( vocab_file=bert_vocab_file, do_lower_case=False)\n",
    "\n",
    "test_reviews = pd.read_excel('../data/nCov_10k_test.xlsx').fillna('')\n",
    "T1,T2 = process(test_reviews, args, tokenizer)\n",
    "\n",
    "k_folds = 5\n",
    "for mode in range(k_folds):\n",
    "\n",
    "    print ('\\n',mode+1)\n",
    "    if mode>0:\n",
    "        tf.reset_default_graph()\n",
    "    model = Classification( args )\n",
    "    model_dir = './model_saved/{}'.format(mode + 1)\n",
    "    sess = tf.Session()\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        raise ValueError('No such file:[{}]'.format(model_dir))\n",
    "\n",
    "    c_ = predict( T1,T2, model )\n",
    "\n",
    "    if mode==0:\n",
    "        rs_c=c_\n",
    "    else:\n",
    "        rs_c+=c_\n",
    "\n",
    "rs_c = rs_c/k_folds*pWeight\n",
    "cc_ = np.argmax(rs_c,1)\n",
    "\n",
    "submit = test_reviews[['微博id']]\n",
    "submit['id'] = submit['微博id']\n",
    "del submit['微博id']\n",
    "submit['y'] = [i-1 for i in cc_]\n",
    "submit[['id','y']].to_csv(output_path, encoding='utf-8', index=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
